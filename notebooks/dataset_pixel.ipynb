{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80f8ded",
   "metadata": {},
   "source": [
    "This is just a simple test notebook for dataset_pixel.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4988d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py: DEVICE is set as cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & project setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust this if the notebook is located elsewhere;\n",
    "# here we assume notebook is one level under project root.\n",
    "project_root = Path(__file__).resolve().parent.parent if '__file__' in globals() else Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common.config import DEVICE, WAVES_DIR, SECTIONS_DIR, RAYS_DIR\n",
    "from common.pmatrix import tensor_pmatrix\n",
    "from common.waveforms import load_all_waveforms\n",
    "from common.dmatrix import load_ray_tensor\n",
    "from models.autoencoder import ConvAutoencoder, DatasetAutoencoder\n",
    "from models.pixel_nn.dataset_pixel import PixelDataset, index_to_triplet, pixel_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea9a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded autoencoder: ConvAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 8, kernel_size=(15,), stride=(5,), padding=(7,))\n",
      "    (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=320, out_features=1024, bias=True)\n",
      "    (8): ELU(alpha=1.0)\n",
      "    (9): Linear(in_features=1024, out_features=32, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=1024, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Unflatten(dim=1, unflattened_size=(16, 20))\n",
      "    (5): ConvTranspose1d(16, 8, kernel_size=(5,), stride=(2,), padding=(2,), output_padding=(1,))\n",
      "    (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): ConvTranspose1d(8, 1, kernel_size=(15,), stride=(5,), padding=(7,), output_padding=(4,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load your autoencoder\n",
    "# ─────────────────────────────────\n",
    "# Adjust CKPT_PATH to point to your trained AE checkpoint\n",
    "CKPT_PATH = project_root / \"artifacts/autoencoder/checkpoints/ConvAE_resample200_lat32_bn_2025-06-20_09-56-52.pt\"\n",
    "\n",
    "# Create a DatasetAutoencoder for initialization\n",
    "wave_ds = DatasetAutoencoder(\n",
    "    path=Path(WAVES_DIR),\n",
    "    reduction=\"resample\",\n",
    "    n=200,\n",
    "    save=False,\n",
    "    force_reload=False\n",
    ")\n",
    "\n",
    "# Load the AE and switch to eval mode\n",
    "ae = ConvAutoencoder.load(\n",
    "    path=CKPT_PATH,\n",
    "    dataset=wave_ds,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "ae.eval()\n",
    "\n",
    "print(\"✅ Loaded autoencoder:\", ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff1939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded waveforms with shape torch.Size([6600, 32]) (num_waveforms, encoding_dims)\n",
      "Sorted files in rays\n",
      "Saved ray tensors shape: torch.Size([100, 66, 30, 20]) (num_sections=100, num_rays=66, nX, nY)\n",
      "C:\\Users\\Jaime\\Documents\\deep-learning-wall-tomography-2\\data\\sections\n",
      "<generator object Path.glob at 0x0000021048994200>\n",
      "Loaded 100 section files with labels shape: torch.Size([100, 20, 30]) (num_sections, nY, nX)\n",
      "✅ PixelDataset created\n",
      "  - # sections: 100\n",
      "  - Total pixels: 60000\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Instantiate PixelDataset\n",
    "# ─────────────────────────────────\n",
    "pixel_ds = PixelDataset(\n",
    "    autoencoder=ae,\n",
    "    nX=30,\n",
    "    nY=20,\n",
    "    save=False   # disable saving during tests\n",
    ")\n",
    "\n",
    "print(\"✅ PixelDataset created\")\n",
    "print(\"  - # sections:\", pixel_ds.num_sections)\n",
    "print(\"  - Total pixels:\", len(pixel_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7d6850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected total pixels: 60000\n",
      "len(pixel_ds): 60000\n",
      "features[0].shape: torch.Size([2182])\n",
      "label[0]: tensor(0.)\n",
      "Batch features shape (first 5): torch.Size([5, 2182])\n",
      "Batch labels shape (first 5): torch.Size([5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaime\\Documents\\deep-learning-wall-tomography-2\\models\\pixel_nn\\dataset_pixel.py:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.labels[p, y, x])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Basic sanity checks\n",
    "# ─────────────────────────────────\n",
    "# 1) Length\n",
    "expected_len = pixel_ds.num_sections * pixel_ds.nX * pixel_ds.nY\n",
    "print(\"Expected total pixels:\", expected_len)\n",
    "print(\"len(pixel_ds):\", len(pixel_ds))\n",
    "\n",
    "# 2) First element\n",
    "features0, label0 = pixel_ds[0]\n",
    "print(\"features[0].shape:\", features0.shape)\n",
    "print(\"label[0]:\", label0)\n",
    "\n",
    "# 4) Test slice\n",
    "batch_feats, batch_labels = pixel_ds[:5]\n",
    "print(\"Batch features shape (first 5):\", batch_feats.shape)\n",
    "print(\"Batch labels shape (first 5):\", batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33726e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Index 0 (section=0, y=0, x=0) ---\n",
      "Feature bounds:      [0.   0.   0.02 0.02]\n",
      "True bounds:         [0.   0.   0.02 0.02]\n",
      "Bounds match?        True\n",
      "\n",
      "First 5 feature distances: [0. 0. 0. 0. 0.]\n",
      "First 5 expected distances: [0. 0. 0. 0. 0.]\n",
      "Distances match?         True\n",
      "\n",
      "Feature label:    0.0\n",
      "Expected label:   0.0\n",
      "Label match?       tensor(True)\n",
      "\n",
      "--- Index 10 (section=0, y=0, x=10) ---\n",
      "Feature bounds:      [0.2  0.   0.22 0.02]\n",
      "True bounds:         [0.2  0.   0.22 0.02]\n",
      "Bounds match?        True\n",
      "\n",
      "First 5 feature distances: [0.         0.         0.         0.04272002 0.02236068]\n",
      "First 5 expected distances: [0.         0.         0.         0.04272002 0.02236068]\n",
      "Distances match?         True\n",
      "\n",
      "Feature label:    1.0\n",
      "Expected label:   1.0\n",
      "Label match?       tensor(True)\n",
      "\n",
      "--- Index 500 (section=0, y=16, x=20) ---\n",
      "Feature bounds:      [0.4  0.32 0.42 0.34]\n",
      "True bounds:         [0.4  0.32 0.42 0.34]\n",
      "Bounds match?        True\n",
      "\n",
      "First 5 feature distances: [0. 0. 0. 0. 0.]\n",
      "First 5 expected distances: [0. 0. 0. 0. 0.]\n",
      "Distances match?         True\n",
      "\n",
      "Feature label:    0.0\n",
      "Expected label:   0.0\n",
      "Label match?       tensor(True)\n",
      "\n",
      "--- Index 59990 (section=99, y=19, x=20) ---\n",
      "Feature bounds:      [0.4  0.38 0.42 0.4 ]\n",
      "True bounds:         [0.4  0.38 0.42 0.4 ]\n",
      "Bounds match?        True\n",
      "\n",
      "First 5 feature distances: [0. 0. 0. 0. 0.]\n",
      "First 5 expected distances: [0. 0. 0. 0. 0.]\n",
      "Distances match?         True\n",
      "\n",
      "Feature label:    0.0\n",
      "Expected label:   0.0\n",
      "Label match?       tensor(True)\n",
      "\n",
      "✅ Logged consistency checks for selected indices.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Consistency checks\n",
    "enc_dim = pixel_ds.encoded_waveforms.size(1)\n",
    "num_rays = pixel_ds.ray_tensors.size(1)\n",
    "\n",
    "for idx in [0, 10, 500, len(pixel_ds) - 10]:\n",
    "    features, label = pixel_ds[idx]\n",
    "    p, y, x = index_to_triplet(idx, pixel_ds.nX, pixel_ds.nY)\n",
    "\n",
    "    # Compute true bounds\n",
    "    true_bounds = pixel_bounds(\n",
    "        x, y,\n",
    "        pixel_ds.nX, pixel_ds.nY,\n",
    "        pixel_ds.Xmin, pixel_ds.Xmax,\n",
    "        pixel_ds.Ymin, pixel_ds.Ymax\n",
    "    )\n",
    "    feat_bounds = features[:4]\n",
    "\n",
    "    # Extract ray distances from feature vector\n",
    "    start = 4\n",
    "    length_per_ray = 1 + enc_dim\n",
    "    ray_block = features[start : start + num_rays * length_per_ray]\n",
    "    ray_block = ray_block.view(num_rays, length_per_ray)\n",
    "    feat_distances = ray_block[:, 0]\n",
    "\n",
    "    # Fetch expected distances\n",
    "    expected_distances = pixel_ds.ray_tensors[p, :, x, y]\n",
    "\n",
    "    # Fetch expected label\n",
    "    expected_label = pixel_ds.labels[p, y, x]\n",
    "\n",
    "    # Log everything\n",
    "    print(f\"\\n--- Index {idx} (section={p}, y={y}, x={x}) ---\")\n",
    "    print(\"Feature bounds:     \", feat_bounds.numpy())\n",
    "    print(\"True bounds:        \", true_bounds.numpy())\n",
    "    print(\"Bounds match?       \", np.allclose(feat_bounds.numpy(), true_bounds.numpy()))\n",
    "\n",
    "    print(\"\\nFirst 5 feature distances:\", feat_distances[:5].numpy())\n",
    "    print(\"First 5 expected distances:\", expected_distances[:5].numpy())\n",
    "    print(\"Distances match?        \", np.allclose(feat_distances.numpy(), expected_distances.numpy()))\n",
    "\n",
    "    print(f\"\\nFeature label:    {label.item()}\")\n",
    "    print(f\"Expected label:   {expected_label}\")\n",
    "    print(\"Label match?      \", label.item() == expected_label)\n",
    "\n",
    "print(\"\\n✅ Logged consistency checks for selected indices.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1dc084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector (first 100 entries) @ idx=0: tensor([ 0.0000e+00,  0.0000e+00,  2.0000e-02,  2.0000e-02,  0.0000e+00,\n",
      "         5.2062e+01,  1.7689e+01,  1.1608e+02, -1.2524e+01,  1.1628e+01,\n",
      "         4.4147e+01, -9.5998e+00,  3.0363e+01, -3.3254e+01,  4.7424e+01,\n",
      "        -2.3140e+01,  1.0771e+02, -7.7548e+01,  2.2487e+01,  1.0268e+02,\n",
      "        -5.0443e+01, -9.2029e+01, -9.7928e+01,  1.5053e+02, -2.7209e+01,\n",
      "         2.5511e+01, -3.4609e+01, -4.6145e+01,  1.5202e+01,  1.2379e+01,\n",
      "        -1.0710e+00, -6.7435e+01,  1.1358e+01,  4.7720e+01,  2.7765e+01,\n",
      "        -4.9607e+01, -3.4104e+01,  0.0000e+00,  1.3982e+00, -6.2443e+00,\n",
      "         3.3082e+01,  7.6359e+00,  9.4067e+00,  4.4811e+01, -2.8527e+01,\n",
      "         1.0355e+02, -4.7832e+01,  1.0069e+02, -2.8791e+01,  1.1360e+02,\n",
      "        -8.2140e+01, -3.0420e+01,  9.7028e+01, -2.9963e+01,  3.0240e+00,\n",
      "        -7.3859e+01,  1.4843e+02, -7.7909e+01,  8.1853e+01,  1.9065e+00,\n",
      "         3.9287e+01, -2.4087e+01,  2.8214e+00, -1.5985e+01,  5.7373e+00,\n",
      "        -4.7275e+00,  1.0312e+02,  2.3157e+01, -7.6463e+01, -4.4900e+01,\n",
      "         0.0000e+00, -2.4135e+01, -2.1374e+01, -2.7643e+01, -1.2537e+01,\n",
      "         2.5867e+01,  2.5330e+01, -9.7826e-01,  5.7483e+01, -3.8450e+01,\n",
      "         1.1138e+02, -8.4749e+01,  5.3181e+01, -5.8977e+01, -1.0419e+01,\n",
      "         6.1004e+01,  4.0549e+00, -1.3974e+00, -1.8960e+00,  1.5294e+02,\n",
      "        -5.0122e+01,  2.7015e+01,  4.1812e+00,  1.1459e+01, -3.5629e+01,\n",
      "        -2.6075e+01, -3.5757e+01,  4.1341e+01, -6.1433e+00,  9.9845e+01])\n",
      "Label @ idx=0: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Quick visual inspection\n",
    "# ─────────────────────────────────\n",
    "idx = 0\n",
    "feat, lbl = pixel_ds[idx]\n",
    "print(f\"Feature vector (first 100 entries) @ idx={idx}:\", feat[:100])\n",
    "print(f\"Label @ idx={idx}:\", lbl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
