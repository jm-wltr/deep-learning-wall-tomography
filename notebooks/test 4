from pathlib import Path
import torch
import matplotlib.pyplot as plt
import sys
import numpy as np

def find_project_root(marker_dir: str = "artifacts") -> Path:
    start = Path(__file__).resolve()
    for parent in (start, *start.parents):
        if (parent / marker_dir).is_dir():
            return parent
    return Path.cwd().resolve()

PROJECT_ROOT = find_project_root("artifacts")
sys.path.insert(0, str(PROJECT_ROOT)) 

from common.config import DEVICE
from models.autoencoder.dataset_autoencoder import DatasetAutoencoder
from models.autoencoder.utils import split_dataset
from models.autoencoder.architectures.flexible_autoencoder import ConvAutoencoder

# 1. Recreate dataset & split
dataset = DatasetAutoencoder(
    path="data/waveforms",
    reduction="resample",
    n=200,
    save=False,
    force_reload=False
)
_, val_set = split_dataset(dataset, train_frac=0.8, seed=42)

# 2. Val DataLoader
val_loader = torch.utils.data.DataLoader(val_set, batch_size=50, shuffle=False)

# 3. Load model
ckpt_path = Path("artifacts/autoencoder/checkpoints") / "ConvAE_resample200_lat32_bn_2025-06-17_16-08-45.pt"
model = ConvAutoencoder(
    dataset=dataset,
    latent_dim=32,
    dropout=0.0,
    use_batchnorm=True,
    batch_size=50,
    learning_rate=1e-3,
    train_frac=0.8,
    seed=42,
    reduction="resample",
    reduction_n=200,
    timestamp=False
).to(DEVICE)
chk = torch.load(ckpt_path, map_location=DEVICE)
model.load_state_dict(chk['state_dict'])
model.eval()

# 4. Run through entire val set, storing originals, reconstructions, and MSE
all_waves = []
all_recons = []
all_mse   = []

with torch.no_grad():
    for batch in val_loader:
        waves = batch.unsqueeze(1).to(DEVICE)           # [B,1,L]
        recon = model(waves).unsqueeze(1).cpu()         # [B,1,L]
        mse   = torch.mean((recon - waves.cpu())**2, dim=[1,2]).numpy()  # [B]

        all_waves.append(waves.cpu().numpy())   # store as numpy [B,1,L]
        all_recons.append(recon.numpy())
        all_mse.append(mse)

# concatenate
all_waves   = np.concatenate(all_waves, axis=0)   # [N,1,L]
all_recons  = np.concatenate(all_recons, axis=0)  # [N,1,L]
mse_per_samp= np.concatenate(all_mse, axis=0)     # [N]

# 5. Find top-20 worst
top20 = np.argsort(mse_per_samp)[-20:][::-1]  # descending order

# 6. Plot in a 5Ã—4 grid
fig, axes = plt.subplots(5, 4, figsize=(16, 12))
for i, idx in enumerate(top20):
    ax = axes.flatten()[i]
    orig = all_waves[idx].squeeze()
    rec  = all_recons[idx].squeeze()
    ax.plot(orig, label='Orig')
    ax.plot(rec,  label='Recon')
    ax.set_title(f"#{i+1}: idx={idx}, MSE={mse_per_samp[idx]:.2e}")
    ax.set_xticks([]); ax.set_yticks([])

# one legend for all
handles, labels = axes[0,0].get_legend_handles_labels()
fig.legend(handles, labels, loc='upper right')
plt.tight_layout()
plt.show()
